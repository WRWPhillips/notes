Kafka Streams ---> What is it?

Kafka streams is a lightweight but powerful Java library for enriching,
transforming, and processing real-time streams of data. 

The Kafka Ecosystem:

	Kafka streams lives among a group of technologies that are
	collectively reffered to as the Kafka ecosystem. The heart of Kafka
	is a distributed append only log that we can produce messages to and
	read messages from. There are many important APIs for interacting
	with this log. Three APIs in the kafka ecosystem are converned with
	the movement of data to and from Kafka. 

	1. Producer API
		Writes messages to topics (filebeat, rsyslog, custom)

	2. Consumer API
		Reads messages from topics (logstash, kafkacat, custom)

	3. Connect API
		Connects external data stores, APIs, and filesystems to Kafka
		topics, involves both reading from topics (sink connectors)
		and writing to topics (source connectors).
		(JDBC source connector, ElasticSearch sink connector, custom)

Enter Streams:

	While moving data through kafka is certainly important for creating 
	data pipelines, some business problems require us to also process
	and react to data as it becomes available in kafka. This is referred
	to as stream processing, and there are multiple ways of building
	stream processing applications with kafka. In the past, these were 
	built on top of the Consumer and Producer APIs with Python, Java, Go,
	C, C++, Node.js, etc. and took a lot of code from scratch in order to
	perform data processing logic. These APIs were very basic and lacked 
	many of the primitives that would qualify them as a stream processing
	api, including:
		
		-Local and fault-tolerant state 
		-A rich set of operators for transforming data streams
		-More advanced representations of streams
		-Sophisticated handling of time

	This meant anything nontrivial like aggregate records, joins between 
	separate streams of data, event grouping into windowed time buckets,
	and ad-hoc queries against streams became very complex very quickly. 
	The producer and consumer APIs did not have any tooling to help with
	these use cases, so things had to be self implemented.
 
	In 2016 Kafka Streams, also known as the streams api, was introduced
	as a solution to these problems. Unlike other APIs Streams is devoted
	to processing real-time data streams, rather than just moving data. 
	
	Streams operates on another layer, a stream processing layer,
	consuming records from the event stream, processing the data, and 
	optionally writing enriched or transformed records back to Kafka.

	Streams provides: 

		- A high-level DSL (domain specific language) that looks and
		feels like Java's streaming API. The DSL provides a fluent
		and functional approach to processing data streams that is
		easy to learn and use.

		- A low-level processor API that gives fine-grained control 
		when needed

		- Convenient abstractions for modeling data as either streams
		or tables

		- The ability to join streams and tables

		- Operators and utilities for building both stateless and 
		stateful stream processing applications

		- Support for time-based operations, including windowing and
		periodic functions.

		- Easy installation. It's a library that can be added to any 
		Java application.

		- Scalability, reliability, and maintainability.

Processor Topologies:

	Kafka Streams leverages a programming paradigm called dataflow
	programming, which is a data-centric method of representing programs
	as a series of inputs, outputs, and processing stages. This leads to 
	a very natural and intuitive way of creating stream processing 
	programs. Instead of building a program as a sequence of steps, the
	stream processing logic in Kafka Streams is structured as a 
	directed acyclic graph. The nodes represent a processing step,
	and the edges represent i/o streams.

			 ________________
			|Source Processor|		
				|
				| <-Stream
			 _______|________
			|Stream Processor|
			    /        \
		Stream->   /          \ <-Stream
			  /            \
		 ______________   ________________ 
		|Sink Processor| |Stream Processor|
					|
					| <-Stream	
				  ______|_______
				 |Sink Processor|

	There are three basic kinds of processors in Kafka Streams:
		
		-Source Processors, sources where information flows into the
		Kafka streams application. Data is read from a Kafka topic
		and sent to one or more stream processors.

		-Stream processors, responsible for applying data processing/
		transformation logic on the input stream. In the high-level
		DSL these are defined using a set of built in operators that 
		are exposed by the Streams library. Filter, map, flatMap,
		join, etc.

		-Sink processors are where enriched, transformed, filtered, 
		or otherwise processed records are written back to kafka
		either to be handled by another stream processing application
		or to be sent to a downstream data store via somethin like 
		Kafka Connect. 

	The collection of these processors forms the processor topology, 
	which is often referred to as /the topology/. This topology provided
	as a Directed Acyclic Graph (DAG) is a neat way to visualize the
	activity of an application. 


	This may be a terrible comparison, but Streams sort of feels like a 
	middleware for Kafka. Take an example application of a chat bot using
	Kafka topics. The chat bot is invoked using @KafkaStreamBot followed
	by a command. Any message invoked using @KafkaStreamBot will go to
	the slack-mentions topic, but the command following it could be
	either valid or invalid. In this case, we could use a simple Stream
	processor called isValid that determines whether a mention is
	followed by a valid command, and then either write to the valid-menti
	ons topic or the invalid-mentions topic depending on whether it
	turned out to have been valid. 

	Sub-topologies: Sub-topologies are basically further alternative
	paths within a processor topology. Taking the example used above for
	example, you'd then be able to route into further processor topologie
	s from the initial one. These can be executed independently, allowing
	for further parallelism. Each of the dotted squares in the diagram 
	below are independently operable sub-topologies. 


             _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
	    		 ________________
	    | 		|Slack Mentions  |		|
				|
	    |			| <-Stream		|
			 _______|________
	    |		|    isValid     |		|
			    /        \			
	    |	Stream->   /          \ <-Stream	|
			  /            \		
	    | _ _ _ _ _ True_  _ _ _ _False_ _ _ _ _ _ _|
	       ______________      ________________ 
	    | |valid-mentions|  | |invalid-mentions|    |
		       |		|
	    |	       |	|	| <-Stream	|
		 ______|_______    _____|___________
	    |	| handleCommand|| |postErrorResponse|   |

	    |_ _ _ _ _ _ _ _ _ _|_ _ _ _ _ _ _ _ _ _ _ _|


	In the case of exchange between sub-topologies, there is no direct
	data movement. isValid posts to a kafka topic that valid-mentions
	consumes from. 

Depth-first processing: 
	
	Kafka Streams uses a depth-first strategy when processing data. When 
	a new record is received, it is routed through each stream processor 
	in the topology before a new record is processed. This means only one
	record at a time is actually moving through the topography. This
	means that slow stream processing operations can block other records 
	from being processed in the same thread. 

Advantages of dataflow programming:

	Representing the program as a directed graph makes it easy to reason
	about. It means there's not a lot of confusing conditionals and 
	control logic, and you only really need to track the nodes and edges
	to know exactly how data is flowing. 

	It also allows us to standardize the way real-time data is framed,
	meaning any kafka streams project will be instantly recognizable and
	familiar to anyone who has worked on any other streams project. 
	Not just because of the reusable abstractions in the library,
	but because of the common approach to problem solving.
	Its nodes (operators) and edges (streams) all the way down. 

	Another benefit is the understandability of the DAG to non-technical
	people, which is great for organizations.

	A final benefit is that the processor topology, which contains the 
	source, sink, and stream processors, acts as a template that can be
	instantiated and parallelized very easily across multiple threads
	and application instances. Being easily replicatable helps with 
	scalability immensely.

Tasks, Stream Threads, Partitions:

	When we define a topology in Kafka Streams we are not actually
	executing the program. Instead we are building a template for how 
	data should flow through our application. This template can be
	instantiated multiple times in a single application instance, and 
	parallelized across many tasks and stream threads.
	There's a close relationship between the number of tasks/threads and 
	the amount of work your stream processing application can handle, so 
	understanding the content in this section is especially important for
	achieving good performance with Kafka Streams. 

	"A task is the smallest unit of work that can be performed in
	parallel in a Kafka Streams apllication... Slightly simplified, the 
	maximum parallelism at which your application may run is bounded by 
	the maximum number of stream tasks, which is bounded by the maximum
	number of partitions of the input topic(s) the application is reading
	from." --Andy Bryant 	

	Basically, the number of tasks you can have is the same number as the
	number of partitions of the topic which has the most partitions that 
	your topology reads from. 

	Tasks are just logical units that are used to instantiate and run 
	processory topologies, but threads are what actually executes it. 
	In Streams, stream threads are designed to be isolated and thread-
	safe. Unlike tasks, there isn't any formula Kafka Streams applies to 
	calculate the number of threads your app should run. Instead, you
	specify the thread count using a configuration property named
	num.stream.threads-- The upper bound for the number of threads you
	can utilize corresponds to the task count, and there are different
	strategies for deciding on the number of stream threads you should
	run with. 

High-Level DSL Versus Low-Level Processor API. 

	A common idea in the software engineering field is that abstraction 
	comes at a cost: the more you abstract details away, the more the 
	software feels like magic, and the more control you give up. 

	Kafka Streams allows developers to choose the abstraction level that
	works best for them by providing two different APIs, the high level 
	DSL and the low level Processor API.

	The high-level DSL is built on top of the processor API but the
	interface is slightly different. It ends up using a more functional
	style of programming, and allows you to leverage higher-level 
	abstractions for dealing with the data (streams and tables). 

	The lower-level Processor API lets you schedule periodic functions,
	gives more granular access to application state, and more fine
	grained control over the timing of certain operations. 


