Kafka Streams ---> What is it?

Kafka streams is a lightweight but powerful Java library for enriching,
transforming, and processing real-time streams of data. 

The Kafka Ecosystem:

	Kafka streams lives among a group of technologies that are
	collectively reffered to as the Kafka ecosystem. The heart of Kafka
	is a distributed append only log that we can produce messages to and
	read messages from. There are many important APIs for interacting
	with this log. Three APIs in the kafka ecosystem are converned with
	the movement of data to and from Kafka. 

	1. Producer API
		Writes messages to topics (filebeat, rsyslog, custom)

	2. Consumer API
		Reads messages from topics (logstash, kafkacat, custom)

	3. Connect API
		Connects external data stores, APIs, and filesystems to Kafka
		topics, involves both reading from topics (sink connectors)
		and writing to topics (source connectors).
		(JDBC source connector, ElasticSearch sink connector, custom)

Enter Streams:
	While moving data through kafka is certainly important for creating 
	data pipelines, some business problems require us to also process
	and react to data as it becomes available in kafka. This is referred
	to as stream processing, and there are multiple ways of building
	stream processing applications with kafka. In the past, these were 
	built on top of the Consumer and Producer APIs with Python, Java, Go,
	C, C++, Node.js, etc. and took a lot of code from scratch in order to
	perform data processing logic. These APIs were very basic and lacked 
	many of the primitives that would qualify them as a stream processing
	api, including:
		
		-Local and fault-tolerant state 
		-A rich set of operators for transforming data streams
		-More advanced representations of streams
		-Sophisticated handling of time

	This meant anything nontrivial like aggregate records, joins between 
	separate streams of data, event grouping into windowed time buckets,
	and ad-hoc queries against streams became very complex very quickly. 
	The producer and consumer APIs did not have any tooling to help with
	these use cases, so things had to be self implemented.
 
	In 2016 Kafka Streams, also known as the streams api, was introduced
	as a solution to these problems. Unlike other APIs Streams is devoted
	to processing real-time data streams, rather than just moving data. 
	
	Streams operates on another layer, a stream processing layer,
	consuming records from the event stream, processing the data, and 
	optionally writing enriched or transformed records back to Kafka.

	Streams provides: 
		- A high-level DSL (domain specific language) that looks and
		feels like Java's streaming API. The DSL provides a fluent
		and functional approach to processing data streams that is
		easy to learn and use.

		- A low-level processor API that gives fine-grained control 
		when needed

		- Convenient abstractions for modeling data as either streams
		or tables

		- The ability to join streams and tables

		- Operators and utilities for building both stateless and 
		stateful stream processing applications

		- Support for time-based operations, including windowing and
		periodic functions.

		- Easy installation. It's a library that can be added to any 
		Java application.

		- Scalability, reliability, and maintainability.

Processor Topologies:

	Kafka Streams leverages a programming paradigm called dataflow
	programming, which is a data-centric method of representing programs
	as a series of inputs, outputs, and processing stages. This leads to 
	a very natural and intuitive way of creating stream processing 
	programs. Instead of building a program as a sequence of steps, the
	stream processing logic in Kafka Streams is structured as a 
	directed acyclic graph. The nodes represent a processing step,
	and the edges represent i/o streams.

			 ________________
			|Source Processor|		
				|
				| <-Stream
			 _______|________
			|Stream Processor|
			    /        \
		Stream->   /          \ <-Stream
			  /            \
		 ______________   ________________ 
		|Sink Processor| |Stream Processor|
					|
					| <-Stream	
				  ______|_______
				 |Sink Processor|

	There are three basic kinds of processors in Kafka Streams:
		
		-Source Processors, sources where information flows into the
		Kafka streams application. Data is read from a Kafka topic
		and sent to one or more stream processors.

		-Stream processors, responsible for applying data processing/
		transformation logic on the input stream. In the high-level
		DSL these are defined using a set of built in operators that 
		are exposed by the Streams library. Filter, map, flatMap,
		join, etc.

		-Sink processors are where enriched, transformed, filtered, 
		or otherwise processed records are written back to kafka
		either to be handled by another stream processing application
		or to be sent to a downstream data store via somethin like 
		Kafka Connect. 

	The collection of these processors forms the processor topology, 
	which is often referred to as /the topology/. This topology provided
	as a Directed Acyclic Graph (DAG) is a neat way to visualize the
	activity of an application. 
